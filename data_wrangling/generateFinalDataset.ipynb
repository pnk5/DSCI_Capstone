{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "generateFinalDataset",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CYqUEUrEvFf0"
      },
      "source": [
        "## generateFinalDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qS53ZJQeqBWv"
      },
      "source": [
        "*   Input:  \n",
        "  1.   *clean_columns_everything.csv*  \n",
        "  2.   *svi_edited.csv* \n",
        "*   Does:  \n",
        "  1.   Data cleaning\n",
        "  2.   Aggregating incidents over census tract\n",
        "  3.   Merging census tracts with ACS and SVI data\n",
        "  4.   Splitting rows into training and test sets\n",
        "  5.   Feature scaling\n",
        "  6.   Creating target variables\n",
        "*   Output:\n",
        "  1.   *trainTractsDroppedStandard.csv*\n",
        "  2.   *testTractsDroppedStandard.csv*\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A_DigTk2nO84"
      },
      "source": [
        "# Install libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "pd.plotting.register_matplotlib_converters()\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import pylab\n",
        "import seaborn as sns\n",
        "import scipy.stats as stats\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "\n",
        "clean_cols = pd.read_csv('data_source/clean_columns_everything.csv', \n",
        "                            parse_dates=True,\n",
        "                            infer_datetime_format=True,\n",
        "                            engine='python'\n",
        "                           )\n",
        "\n",
        "svi_cols = pd.read_csv('data_source/svi_edited.csv', \n",
        "                            engine='python'\n",
        "                           )\n",
        "\n",
        "# Allow us to see all columns of our dataframe\n",
        "pd.set_option('max_columns', None)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z1z7sRlhndQA"
      },
      "source": [
        "## Data Cleaning and creating 'week' utility column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JH2blnCpnYdU"
      },
      "source": [
        "# Remove duplicates\n",
        "clean_cols['duplicate_identifier'] = clean_cols['group'] + clean_cols['incident_id']\n",
        "clean_cols.drop_duplicates(subset=['duplicate_identifier'], inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vnGwk10pnftP"
      },
      "source": [
        "# Replace NaN in NFIRS codes with -999\n",
        "replace_dict = {'nfirs_group_final': {np.nan:'-999', 'U':'999'}, \n",
        "                'nfirs_category_final': {np.nan:'-999', 'UU':'999'}, \n",
        "                'nfirs_code_final': {np.nan:'-999', 'UUU':'999'}}\n",
        "clean_cols.replace(replace_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lvO5lUyTnsTF"
      },
      "source": [
        "# Convert 'occurred_on' dtype from 'object' --> 'datetime'\n",
        "clean_cols['occurred_on'] = pd.to_datetime(clean_cols['occurred_on'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Convert NFIRS code column dtypes from 'object' --> 'int16'\n",
        "clean_cols['nfirs_group_final'] = clean_cols['nfirs_group_final'].astype('int16')\n",
        "clean_cols['nfirs_category_final'] = clean_cols['nfirs_category_final'].astype('int16')\n",
        "clean_cols['nfirs_code_final'] = clean_cols['nfirs_code_final'].astype('int16')\n",
        "\n",
        "# Replace 0 --> NaN so that it can be imputed\n",
        "clean_cols.replace({'median_year_built': {0.0: np.nan}}, inplace=True)\n",
        "\n",
        "# Round values to two decimal places\n",
        "clean_cols = clean_cols.round(2)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0LCmVVQnt-y"
      },
      "source": [
        "# Create a temporary column containing values denoting week in the year (1-52)\n",
        "temp_col = clean_cols['occurred_on'].dt.isocalendar().week\n",
        "\n",
        "# Insert the column after \"day\" at index position 62\n",
        "clean_cols.insert(62, 'week', temp_col)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgHZ1TYGn4E_"
      },
      "source": [
        "## Aggregating total, medical, and fire incident counts over census tract"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3rnnReVjn7gT"
      },
      "source": [
        "# Create dataframe of total incidents grouped by TRACT\n",
        "groupByTract = clean_cols.groupby(['TRACTA']).size().reset_index(name='total_incidents')\n",
        "\n",
        "# Add column of total medical incidents grouped by TRACT\n",
        "medicalGroupByTract = clean_cols[clean_cols['nfirs_group_final'] == 3].groupby(['TRACTA']).size().reset_index(name='medical_incidents')\n",
        "groupByTract = groupByTract.merge(medicalGroupByTract, on='TRACTA', how='left')\n",
        "\n",
        "# Add column of total fire incidents grouped by TRACT\n",
        "fireGroupByTract = clean_cols[clean_cols['nfirs_group_final'] == 1].groupby(['TRACTA']).size().reset_index(name='fire_incidents')\n",
        "groupByTract = groupByTract.merge(fireGroupByTract, on='TRACTA', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cQJrWuYvn-vA"
      },
      "source": [
        "## Finding the number of days a census tract has \"been in the system\"¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vbNl6SOMn_DB"
      },
      "source": [
        "# Figuring out when was earliest report by tract\n",
        "earliestTractIncident = clean_cols.groupby('TRACTA')['occurred_on'].agg(['min'], on='occurred_on').rename({'min':'earliest_incident'},axis=1).reset_index()\n",
        "\n",
        "# Figuring out when was latest report by tract\n",
        "latestTractIncident = clean_cols.groupby('TRACTA')['occurred_on'].agg(['max'], on='occurred_on').rename({'max':'latest_incident'},axis=1).reset_index()\n",
        "\n",
        "# Add columns showing when census tract's first and last incident appeared in our dataset\n",
        "groupByTract = groupByTract.merge(earliestTractIncident, on='TRACTA', how='left')\n",
        "groupByTract = groupByTract.merge(latestTractIncident, on='TRACTA', how='left')\n",
        "\n",
        "# Convert columns to datetime\n",
        "groupByTract['earliest_incident'] = pd.to_datetime(groupByTract['earliest_incident'], format='%Y-%m-%d %H:%M:%S')\n",
        "groupByTract['latest_incident'] = pd.to_datetime(groupByTract['latest_incident'], format='%Y-%m-%d %H:%M:%S')\n",
        "\n",
        "# Add column for the number of days that elapsed between 'earliest_incident' and 'latest_incident'\n",
        "groupByTract['diffTractFirstLastInc'] = (groupByTract['latest_incident'] - groupByTract['earliest_incident']).dt.days\n",
        "\n",
        "# Replace '0' with '1' in diffTractFirstLastInc\n",
        "groupByTract.replace(to_replace={'diffTractFirstLastInc': {0: 1}}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyjKSdx8oJij"
      },
      "source": [
        "## Add indicator variables flagging mis-reported census tracts¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E8j92hvdoNcL"
      },
      "source": [
        "# Create helper column that will help us identify mis-reported census tracts\n",
        "groupByTract['logTotalIncidents'] = np.log(groupByTract['total_incidents'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qV2ye3FeoVUQ"
      },
      "source": [
        "# Create indicator variable flagging first mis-represented group of census tracts\n",
        "groupByTract['firstTroubleGroup'] = ((groupByTract['diffTractFirstLastInc'] > 425) & \n",
        "                                    (groupByTract['diffTractFirstLastInc'] < 475) & \n",
        "                                    (groupByTract['logTotalIncidents'] > 3.5)).astype(int)\n",
        "\n",
        "# Create indicator variable flagging second mis-represented group of census tracts\n",
        "groupByTract['secondTroubleGroup'] = ((groupByTract['diffTractFirstLastInc'] > 700) &\n",
        "                                     (groupByTract['diffTractFirstLastInc'] < 750) &\n",
        "                                     (groupByTract['logTotalIncidents'] > 3.5)).astype(int)\n",
        "\n",
        "# Create indicator variable flagging third mis-represented group of census tracts\n",
        "groupByTract['thirdTroubleGroup'] = ((groupByTract['diffTractFirstLastInc'] > 1000) &\n",
        "                                    (groupByTract['diffTractFirstLastInc'] < 1100) &\n",
        "                                    (groupByTract['logTotalIncidents'] > 4.5)).astype(int)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kD8Yz19moYA9"
      },
      "source": [
        "## Collecting demographics to supplement census tract aggregate incident data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "chukh0xWobz4"
      },
      "source": [
        "# Columns to drop that don't represent a census tract\n",
        "drop_cols = ['Unnamed: 0','id', 'nfirs_group_final', 'incident_id', 'alternate_id', \n",
        "            'duplicate_identifier', 'nfirs_category_final', 'nfirs_code_final',\n",
        "            'occurred_on', 'week', 'day', 'month', 'year', 'hour_of_day', \n",
        "            'day_of_week', 'day_of_month', 'day_of_year', 'month_of_year',\n",
        "            'zip_code_lookup']\n",
        "\n",
        "# Get unique census tracts from incident data and drop columns relevant to incident but not to census tract\n",
        "uniqueCensusTracts = clean_cols.drop_duplicates(subset=['TRACTA'], inplace=False)\n",
        "uniqueCensusTracts = uniqueCensusTracts.drop(columns=drop_cols)\n",
        "\n",
        "# Generate subsets of uniqueCensusTracts so that we can rearrange the dataframe in a more intutitive way\n",
        "censusCoreInfo = uniqueCensusTracts.loc[:, 'group':'TRACTA']\n",
        "censusPopStats = uniqueCensusTracts.loc[:, 'total_pop':'percent_2.01ormore_per_room']\n",
        "censusPopStats2 = uniqueCensusTracts.loc[:, 'median_age':'percent_over90min_commute']\n",
        "\n",
        "# Rearrange the dataframe\n",
        "uniqueTractsRearranged = pd.concat([censusCoreInfo, censusPopStats, censusPopStats2], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q0SyN4I3of6S"
      },
      "source": [
        "## Create base dataset of census tracts, their demographics, and their aggregated incidents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlTClTWdolrd"
      },
      "source": [
        "########### Merge (census tract aggregate incident data) with (demographic data) ###########\n",
        "\n",
        "\n",
        "# Merge dataframes to collect the info of unique census tracts (from both), \n",
        "# aggregate incident counts (from 'groupByTract'), and demographic data (from 'uniqueTractsRearranged')\n",
        "censusTractDemographics = groupByTract.merge(uniqueTractsRearranged, on='TRACTA', how='left')\n",
        "\n",
        "\n",
        "########### Remove census tracts that don't contain any demographic data ###########\n",
        "\n",
        "# Get census tracts that should be removed (contain many NaN or demographics that are nonsensical)\n",
        "dropSet1 = set(censusTractDemographics[censusTractDemographics.loc[:,'percent_male':'percent_family'].isnull().all(axis=1)].TRACTA.tolist())\n",
        "dropSet2 = set(censusTractDemographics[censusTractDemographics.loc[:,'percent_under_0.5_IPL':'median_household_income'].isnull().all(axis=1)].TRACTA.tolist())\n",
        "dropSetMerged = dropSet1.union(dropSet2)\n",
        "dropSetMerged.add(12300)\n",
        "\n",
        "# Update dataset to remove these census tracts\n",
        "censusTractDemographics = censusTractDemographics[~censusTractDemographics['TRACTA'].isin(dropSetMerged)]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gWd_Jgpcoqxe"
      },
      "source": [
        "## Create target variables"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tT-igfqEoubl"
      },
      "source": [
        "# Create target variables normalized by population\n",
        "censusTractDemographics['incidents_per_capita'] = censusTractDemographics['total_incidents'] / censusTractDemographics['total_pop']\n",
        "censusTractDemographics['medical_per_capita'] = censusTractDemographics['medical_incidents'] / censusTractDemographics['total_pop']\n",
        "censusTractDemographics['fire_per_capita'] = censusTractDemographics['fire_incidents'] / censusTractDemographics['total_pop']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDkXW-ceoymR"
      },
      "source": [
        "## Join SVI Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAqdTzoqo0Uw"
      },
      "source": [
        "# Join data on 'GISJOIN'\n",
        "censusTractDemographics = censusTractDemographics.merge(svi_cols, on='GISJOIN', how='left')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8Mziw48o3SD"
      },
      "source": [
        "## Remove utility / useless columns¶"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yLpraXd3o7Uu"
      },
      "source": [
        "# Specify columns to remove\n",
        "remove_cols = ['medical_incidents', 'fire_incidents', \n",
        "               'earliest_incident', 'latest_incident', 'diffTractFirstLastInc',\n",
        "               'group', 'agency', 'tract_block_group_centroid_lat', 'tract_block_group_centroid_lng',\n",
        "               'GISJOIN', 'COUNTY', 'Unnamed: 0', 'logTotalIncidents', 'percent_1_person_hh']\n",
        "\n",
        "# Drop the columns\n",
        "censusTractDemographics.drop(columns=remove_cols, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7FTbHNIpBeq"
      },
      "source": [
        "## Drop \"Trouble\" Tracts"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wlbdmtv6pbm2"
      },
      "source": [
        "# Get \"Trouble\" Tracts for analysis\n",
        "droppedTroubleTracts = censusTractDemographics[(censusTractDemographics.firstTroubleGroup == 1) | \n",
        "                                                  (censusTractDemographics.secondTroubleGroup == 1) |\n",
        "                                                  (censusTractDemographics.thirdTroubleGroup == 1)]\n",
        "\n",
        "# Drop \"Trouble\" Tracts\n",
        "censusTractDemographics = censusTractDemographics[(censusTractDemographics.firstTroubleGroup == 0) & \n",
        "                                                  (censusTractDemographics.secondTroubleGroup == 0) &\n",
        "                                                  (censusTractDemographics.thirdTroubleGroup == 0)]\n",
        "\n",
        "\n",
        "censusTractDemographics.drop(columns=['firstTroubleGroup',\n",
        "                                      'secondTroubleGroup',\n",
        "                                      'thirdTroubleGroup'] , inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZYWYLI85phBE"
      },
      "source": [
        "## Split the Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHXsdJ7opke0"
      },
      "source": [
        "# 80-20 Train-Test Split\n",
        "trainTracts, testTracts = train_test_split(censusTractDemographics, \n",
        "                                           train_size=0.8, test_size=0.2, random_state=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yimbh62ApnEZ"
      },
      "source": [
        "## Impute Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6L1cgntqpp9d"
      },
      "source": [
        "# Get the medians of the train and test sets\n",
        "trainMedianMedianYearBuilt = trainTracts['median_year_built'].median()\n",
        "testMedianMedianYearBuilt = testTracts['median_year_built'].median()\n",
        "\n",
        "# Impute missing values in 'median_year_built' \n",
        "trainTracts.replace({'median_year_built': {np.nan: trainMedianMedianYearBuilt}}, inplace=True)\n",
        "testTracts.replace({'median_year_built': {np.nan: testMedianMedianYearBuilt}}, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Gtg5_l6CpsQl"
      },
      "source": [
        "## One-Hot Encode the 'STATE' Column"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AOUY5wbBpzJc"
      },
      "source": [
        "from sklearn.preprocessing import LabelBinarizer\n",
        "\n",
        "# Apply one-hot encoder to the relevant columns\n",
        "OH_encoder = LabelBinarizer()\n",
        "OH_cols_train = pd.DataFrame(OH_encoder.fit_transform(trainTracts['STATE']))\n",
        "OH_cols_test = pd.DataFrame(OH_encoder.transform(testTracts['STATE']))\n",
        "\n",
        "# Replace default column names with more descriptive ones\n",
        "OH_cols_train.columns = OH_encoder.classes_\n",
        "OH_cols_test.columns = OH_encoder.classes_\n",
        "\n",
        "# One-hot encoding removed index; put it back\n",
        "OH_cols_train.index = trainTracts.index\n",
        "OH_cols_test.index = testTracts.index\n",
        "\n",
        "# Remove categorical columns (will replace with one-hot encoding)\n",
        "trainTracts.drop('STATE', axis=1, inplace=True)\n",
        "testTracts.drop('STATE', axis=1, inplace=True)\n",
        "\n",
        "# Add one-hot encoded columns to numerical features\n",
        "trainTracts = pd.concat([trainTracts, OH_cols_train], axis=1)\n",
        "testTracts = pd.concat([testTracts, OH_cols_test], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3-SXJudWp0rb"
      },
      "source": [
        "## Convert NaN --> 0 for medical_per_capita and fire_per_capita target colums"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f8BmQpUxp4b9"
      },
      "source": [
        "# Replace NaN in 0\n",
        "replace_dict = {'medical_per_capita': {np.nan:0}, \n",
        "                'fire_per_capita': {np.nan:0}}\n",
        "\n",
        "trainTracts.replace(replace_dict, inplace=True)\n",
        "testTracts.replace(replace_dict, inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U7_j_RVwp7SV"
      },
      "source": [
        "## Normalize Features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "40WLPfMop-wP"
      },
      "source": [
        "# Function to generate a kdeplot and Q-Q plot of a given feature\n",
        "def normalizeHelper(df, feature):  \n",
        "    # kdeplot on the left\n",
        "    plt.figure(figsize=(15,4))\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.title(feature, fontsize=15)\n",
        "    sns.kdeplot(df[feature], shade=True)\n",
        "\n",
        "    # Q-Q plot on the right\n",
        "    plt.subplot(1, 2, 2)\n",
        "    stats.probplot(df[feature], dist=\"norm\", plot=pylab)\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# # To visualize the distributions of our features\n",
        "# for feature in trainTracts.loc[:,'total_pop':'SPL_THEMES'].columns:\n",
        "#     normalizeHelper(trainTracts, feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "97zii0tcqkKg"
      },
      "source": [
        "# Get columns that already exhibit a Gaussian distribution\n",
        "colAlreadyGaussian = ['percent_male', 'median_year_built',\n",
        "                      'percent_0to19_years', 'percent_40to64_years', 'percent_employer_only',\n",
        "                      'percent_2_person_hh', 'percent_3_person_hh', 'percent_4_person_hh', \n",
        "                      'percent_0.5to1_per_room', 'percent_2.01ormore_per_room',\n",
        "                      'median_age', 'percent_leave_for_work_6to9am',\n",
        "                      'percent_15to24min_commute', 'percent_25to34min_commute', 'SPL_THEME2',\n",
        "                      'RPL_THEME3', 'SPL_THEME4', 'SPL_THEMES']\n",
        "\n",
        "colsToNormalize = list(trainTracts.loc[:,'total_pop':'SPL_THEMES'].columns)\n",
        "colsToNormalize.remove('incidents_per_capita')\n",
        "colsToNormalize.remove('medical_per_capita')\n",
        "colsToNormalize.remove('fire_per_capita')\n",
        "\n",
        "# Get columns that do NOT already exhibit a Guassian distribution\n",
        "nonGaussianCols = [col for col in colsToNormalize if not col in colAlreadyGaussian]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9sNYiwpeqvys"
      },
      "source": [
        "## SQRT Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pnTFWA5mqxma"
      },
      "source": [
        "colsToSqrt = ['total_pop', 'percent_20to39_years', 'percent_65andover', 'percent_multi_race',\n",
        "              'percent_hispanic', 'percent_living_alone', 'percent_nonfamily_not_alone',\n",
        "              'percent_graduate_degree', 'percent_under_0.5_IPL', 'percent_0.5to0.99_IPL',\n",
        "              'percent_1to1.24_IPL', 'percent_1.25to1.49_IPL', 'percent_1.5to1.84_IPL',\n",
        "              'percent_1.85to1.99_IPL', 'percent_ue_in_labor_force', 'percent_no_health_insur',\n",
        "              'percent_direct_purchase_only', 'percent_medicare_only', 'percent_medicaid_only',\n",
        "              'percent_renter_occ', 'percent_5_person_hh', 'percent_leave_for_work_before_6am',\n",
        "              'percent_leave_for_work_12to4pm', 'percent_leave_for_work_4tomidnight',\n",
        "              'percent_5to14min_commute', 'percent_35to44min_commute', 'percent_45to59min_commute',\n",
        "              'percent_60to89min_commute', 'percent_over90min_commute','EP_LIMENG', \n",
        "              'RPL_THEME1', 'RPL_THEME4', 'RPL_THEMES', 'SPL_THEME1']\n",
        "\n",
        "# Create columns formed by taking the square root\n",
        "def sqrtTransform(df, feature):\n",
        "    df[feature + '_' + 'sqrt'] = np.sqrt(df[feature])\n",
        "    return normalizeHelper(df, feature + '_' + 'sqrt')\n",
        "\n",
        "# Add SQRT-transformed features to trainTracts\n",
        "for feature in colsToSqrt:\n",
        "    sqrtTransform(trainTracts, feature)\n",
        "    \n",
        "\n",
        "# Add SQRT-transformed features to testTracts\n",
        "for feature in colsToSqrt:\n",
        "    sqrtTransform(testTracts, feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QYjIDuHUq0zd"
      },
      "source": [
        "## LOG Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mhsq5uDIq-Y3"
      },
      "source": [
        "colsToLog = ['median_household_income', 'percent_combo_health_insur']\n",
        "\n",
        "# Create columns formed by taking the natural log\n",
        "def logTransform(df, feature):\n",
        "    df[feature + '_' + 'log'] = np.log(df[feature])\n",
        "    return normalizeHelper(df, feature + '_' + 'log')\n",
        "\n",
        "# Add LOG-transformed features to trainTracts\n",
        "for feature in colsToLog:\n",
        "    logTransform(trainTracts, feature)\n",
        "    \n",
        "# Add LOG-transformed features to testTracts\n",
        "for feature in colsToLog:\n",
        "    logTransform(testTracts, feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swL0jatfrCDr"
      },
      "source": [
        "## BOXCOX Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YrhHFT_6rFM1"
      },
      "source": [
        "colsToBoxCox = ['percent_family', 'percent_bachelors_degree_or_higher', 'percent_over_2_IPL',\n",
        "                'percent_in_labor_force', 'num_housing_units', 'percent_under0.5_per_room', \n",
        "                'percent_drive_to_work']\n",
        "\n",
        "# Create columns formed by taking the box-cox transform\n",
        "def boxCoxTransform(df, feature):\n",
        "    df[feature + '_' + 'boxcox'], param = stats.boxcox(df[feature])\n",
        "    return normalizeHelper(df, feature + '_' + 'boxcox')\n",
        "\n",
        "# Add BOXCOX-transformed features to trainTracts\n",
        "for feature in colsToBoxCox:\n",
        "    boxCoxTransform(trainTracts, feature)\n",
        "    \n",
        "# Add BOXCOX-transformed features to testTracts\n",
        "for feature in colsToBoxCox:\n",
        "    boxCoxTransform(testTracts, feature)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jes1l85wrHer"
      },
      "source": [
        "## EXPONENTIAL Transform"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N0NJp0FxrM1W"
      },
      "source": [
        "colsToExponent = ['percent_white_alone', 'percent_black_alone', 'percent_asian_alone',\n",
        "                  'percent_other_race_alone', 'percent_HS_diploma_or_higher','percent_some_college_or_higher',\n",
        "                  'percent_w_public_assist_income', 'percent_vacant', 'percent_tricare_only', 'percent_VA_only',\n",
        "                  'percent_6_person_hh', 'percent_7ormore_person_hh', 'percent_1.01to1.5_per_room',\n",
        "                  'percent_1.51to2_per_room', 'percent_walk_or_bike_to_work', 'percent_public_transpo_to_work', \n",
        "                  'percent_under5min_commute', 'EP_GROUPQ', 'RPL_THEME2', 'SPL_THEME3']\n",
        "\n",
        "\n",
        "# Transform the remaining, hard-to-transform columns to a Gaussian distribution by raising to a power\n",
        "def exponentialTransform(df, feature, powerOf):\n",
        "    df[feature + '_' + 'exp' + str(powerOf)] = df[feature]**(powerOf)\n",
        "    return normalizeHelper(df, feature + '_' + 'exp' + str(powerOf))\n",
        "\n",
        "# Add EXPONENT-transformed features to trainTracts\n",
        "exponentialTransform(trainTracts, 'percent_white_alone', (1.4))\n",
        "exponentialTransform(trainTracts, 'percent_black_alone', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_asian_alone', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_other_race_alone', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_HS_diploma_or_higher', (3.6))\n",
        "exponentialTransform(trainTracts, 'percent_some_college_or_higher', (1.5))\n",
        "exponentialTransform(trainTracts, 'percent_w_public_assist_income', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_vacant', (0.2))\n",
        "exponentialTransform(trainTracts, 'percent_tricare_only', (0.7))\n",
        "exponentialTransform(trainTracts, 'percent_VA_only', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_6_person_hh', (0.4))\n",
        "exponentialTransform(trainTracts, 'percent_7ormore_person_hh', (0.4))\n",
        "exponentialTransform(trainTracts, 'percent_1.01to1.5_per_room', (0.4))\n",
        "exponentialTransform(trainTracts, 'percent_1.51to2_per_room', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_walk_or_bike_to_work', (0.3))\n",
        "exponentialTransform(trainTracts, 'percent_public_transpo_to_work', (0.4))\n",
        "exponentialTransform(trainTracts, 'percent_under5min_commute', (0.3))\n",
        "exponentialTransform(trainTracts, 'EP_GROUPQ', (0.2))\n",
        "exponentialTransform(trainTracts, 'RPL_THEME2', (0.6))\n",
        "exponentialTransform(trainTracts, 'SPL_THEME3', (1.2))\n",
        "\n",
        "\n",
        "# Add EXPONENT-transformed features to testTracts\n",
        "exponentialTransform(testTracts, 'percent_white_alone', (1.4))\n",
        "exponentialTransform(testTracts, 'percent_black_alone', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_asian_alone', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_other_race_alone', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_HS_diploma_or_higher', (3.6))\n",
        "exponentialTransform(testTracts, 'percent_some_college_or_higher', (1.5))\n",
        "exponentialTransform(testTracts, 'percent_w_public_assist_income', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_vacant', (0.2))\n",
        "exponentialTransform(testTracts, 'percent_tricare_only', (0.7))\n",
        "exponentialTransform(testTracts, 'percent_VA_only', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_6_person_hh', (0.4))\n",
        "exponentialTransform(testTracts, 'percent_7ormore_person_hh', (0.4))\n",
        "exponentialTransform(testTracts, 'percent_1.01to1.5_per_room', (0.4))\n",
        "exponentialTransform(testTracts, 'percent_1.51to2_per_room', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_walk_or_bike_to_work', (0.3))\n",
        "exponentialTransform(testTracts, 'percent_public_transpo_to_work', (0.4))\n",
        "exponentialTransform(testTracts, 'percent_under5min_commute', (0.3))\n",
        "exponentialTransform(testTracts, 'EP_GROUPQ', (0.2))\n",
        "exponentialTransform(testTracts, 'RPL_THEME2', (0.6))\n",
        "exponentialTransform(testTracts, 'SPL_THEME3', (1.2))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "66yBAXgLrSuM"
      },
      "source": [
        "## Move Columns Around (For Convenience / Viewing Purposes)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vOtyr7UrrYRv"
      },
      "source": [
        "# Get the 'parent' (nonGaussianCols) and 'child' (colNewlyGaussian) features\n",
        "nonGaussianCols = [col for col in colsToNormalize \n",
        "                   if not col in colAlreadyGaussian]\n",
        "colNewlyGaussian = trainTracts.loc[:, 'total_pop_sqrt':'SPL_THEME3_exp1.2'].columns\n",
        "\n",
        "\n",
        "# Drop non-Gaussian columns from trainTracts and append them to the end of the dataset\n",
        "trainTractsNonGaussian = trainTracts[nonGaussianCols]\n",
        "trainTracts.drop(columns=nonGaussianCols, inplace=True)\n",
        "trainTracts = pd.concat([trainTracts, trainTractsNonGaussian], axis=1)\n",
        "\n",
        "# Drop non-Gaussian columns from testTracts and append them to the end of the dataset\n",
        "testTractsNonGaussian = testTracts[nonGaussianCols]\n",
        "testTracts.drop(columns=nonGaussianCols, inplace=True)\n",
        "testTracts = pd.concat([testTracts, testTractsNonGaussian], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_97ZlAzZrfPl"
      },
      "source": [
        "targetCols = ['incidents_per_capita', 'medical_per_capita', 'fire_per_capita']\n",
        "\n",
        "# Drop target column from trainTracts and append to the end of the dataset\n",
        "trainTractsTarget = trainTracts[targetCols]\n",
        "trainTracts.drop(columns=targetCols, inplace=True)\n",
        "trainTracts = pd.concat([trainTracts, trainTractsTarget], axis=1)\n",
        "\n",
        "# Drop target columns from testTracts and append to the end of the dataset\n",
        "testTractsTarget = testTracts[targetCols]\n",
        "testTracts.drop(columns=targetCols, inplace=True)\n",
        "testTracts = pd.concat([testTracts, testTractsTarget], axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GJIAYnpzrian"
      },
      "source": [
        "## Output Data to .CSV"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "axr-Hf9mrlfW"
      },
      "source": [
        "# Output splits of aggregated census tracts to .csv\n",
        "trainTracts.to_csv('data_source/trainTractsDroppedStandard.csv')\n",
        "testTracts.to_csv('data_source/trainTractsDroppedStandard.csv')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}